# The CLunch talks can be added to this file. They will appear on the website
# in the order that they appear here, so they should be listed in
# reverse-chronological order.
#
# This should contain speakers for the current semester (that have already spoken, upcoming speakers
# go in future_clunch.yml), and previous semester.
#
# Here is the template for an entry:
#
# - speaker: Name of the Speaker
#   url: http://speakers-website.com
#   affiliation: University Name
#   date: June 26, 2019
#   title: The talk of the title goes here.
#   abstract: The abstract of the talk will go here.

- speaker: Soroush Vosoughi
  url: https://www.cs.dartmouth.edu/~soroush/
  img: /assets/img/clunch/soroush_vosoughi.jpeg
  affiliation: Dartmouth College
  date: December 12, 2022
  title: "Prosocial Language Models"
  abstract: |
     Large-scale language models. (e.g., BERT, GPT-3) have revolutionized the field of natural language processing (NLP). Such pre-trained models show close-to-human-level performance on diverse tasks with little or no training data. The success of such models is at least partially due to their large size (most have hundreds or even thousands of millions of parameters) and the large datasets used for their pre-training (typically collected from the web). However, these same attributes lead to these models reflecting the biases and the antisocial attitudes on the web. These attitudes are a significant bottleneck for using these models in real-world settings, especially for social applications. In my lab, we develop methods for post hoc (i.e., inference time) mitigation of such antisocial attitudes. Post hoc mitigation allows us to avoid retraining the models (which is costly and intractable) while enforcing prosocial attitudes during inference. In this talk, I will review some of our recent work for making language models less biased and more aligned with human moral values through inference-time mitigation.

- speaker: Ben Van Durme
  url: https://www.cs.jhu.edu/~vandurme/
  img: /assets/img/clunch/ben_van_durme.jpeg
  affiliation: Johns Hopkins University
  date: December 5, 2022
  title: "Embracing Uncertainty"
  abstract: |
     I will discuss a series of projects on collecting labels with uncertainty. Time allowing I will touch on model calibration and downstream tasks.
     Modern Artificial Intelligence rests heavily on probabilistic models for classification. This usually means categorical nominal assignment; discrete labels given to inputs at prediction time. For example, an object captured in an image either was or was not truly a "cat", or some text describes an event that we might describe as a "TRANSACTION".
     While concepts like cats and transactions can be real in the world, this does not mean agents can be certain of these truths.  In practice, human agents (annotators) are forced to choose from a label set without reflecting uncertainty in their decisions, and modelers then force artificial agents to do the same. Blurry images and ambiguous texts lead humans to have uncertain beliefs, while modern neural frameworks trained on discrete labels make predictions with high confidence.  Lets instead embrace uncertainty as part of agent (task) design.

- speaker: Smaranda Muresan
  url: http://www.cs.columbia.edu/~smara/
  img: /assets/img/clunch/smara.jpeg
  affiliation: Columbia University
  date: November 28, 2022
  title: "Text Generation: The Curious Case of Figurative Language and Argumentation"
  abstract: |
     Large-scale language models based on transformer architectures, such as GPT-3 or BERT, have advanced the state of the art in Natural Language Understanding and Generation. However, even though these models have shown impressive performance for a variety of tasks, they often struggle to model implicit and/or non-compositional meaning, such as figurative language and argumentative text. In this talk, I will present some of our work on text generation models for figurative language and argumentation. There are two main challenges we have to address to make progress in this space: 1) the need to model common sense and/or connotative knowledge required for these tasks; and 2) the lack of large training datasets. I will discuss our proposed theoretically-grounded knowledge-enhanced text generation models for figurative language such as metaphor and for argument reframing. If time permits I will share our recent efforts of using a model-in-the-loop approach for building datasets for figurative language understanding modeled as an entailment task with explanation generation.

- speaker: Yulia Tsvetkov
  url: https://homes.cs.washington.edu/~yuliats/
  img: /assets/img/clunch/yulia_tsvetkov.jpeg
  affiliation: University of Washington
  date: November 21, 2022
  title: "Interpretation as Weak Supervision for Data-Efficient NLP"
  abstract: |
     Deep learning is typically associated with an abundance of data. But there are scenarios when pre-collected data will never be enough. For example, language on social media is constantly evolving and pretrained language models cannot adapt to rapid language change, dialects, and sociolects, no matter how large pretraining/annotated datasets are. Other examples of constantly evolving and therefore always-low-resource language domains include scientific articles, expert notes, and even news.
     In this talk, I will advocate for using model interpretability methods to dynamically procure data annotations in such low resource scenarios.  In the first part, I will show how instance attribution approaches to model interpretability can identify critical training examples to improve the robustness and adaptability of hate speech classifiers. In the second part, I'll show how self-explaining models can be used for entity and keyphrase extraction in scientific articles. I'll conclude with more ideas for this new paradigm of using approaches to interpreting neural networks as an intrinsic component in low-resource NLP systems and not only as a tool to present explanations to humans.


- speaker: Yulia Tsvetkov
  url: https://homes.cs.washington.edu/~yuliats/
  img: /assets/img/clunch/yulia_tsvetkov.jpeg
  affiliation: University of Washington
  date: November 21, 2022
  title: "Interpretation as Weak Supervision for Data-Efficient NLP"
  abstract: |
     Deep learning is typically associated with an abundance of data. But there are scenarios when pre-collected data will never be enough. For example, language on social media is constantly evolving and pretrained language models cannot adapt to rapid language change, dialects, and sociolects, no matter how large pretraining/annotated datasets are. Other examples of constantly evolving and therefore always-low-resource language domains include scientific articles, expert notes, and even news.
     In this talk, I will advocate for using model interpretability methods to dynamically procure data annotations in such low resource scenarios.  In the first part, I will show how instance attribution approaches to model interpretability can identify critical training examples to improve the robustness and adaptability of hate speech classifiers. In the second part, I'll show how self-explaining models can be used for entity and keyphrase extraction in scientific articles. I'll conclude with more ideas for this new paradigm of using approaches to interpreting neural networks as an intrinsic component in low-resource NLP systems and not only as a tool to present explanations to humans.

- speaker: Rotem Dror
  url: https://rtmdrr.github.io/https://rtmdrr.github.io/
  img: /assets/img/clunch/rotem_dror.jpeg
  affiliation: University of Pennsylvania
  date: November 14, 2022
  title: "Standards for Experiment Design and Evaluation in Natural Language Processing"
  abstract: |
     In this job-talk-like seminar, I will present selected works from my Ph.D. and postdoctoral research. In the first part of the talk, I will overview three papers that cover practices to compare two NLP models and decide which is better based on experiment practices that are prevalent in NLP, such as conducting experiments with multiple datasets and deep neural network models. In the second part of the talk, I will dive into the intriguing world of evaluation of text-generation applications, where I will discuss how to determine which automatic evaluation metrics are appropriate.

- speaker: Hannaneh Hajishirzi
  url: https://homes.cs.washington.edu/~hannaneh/
  img: /assets/img/clunch/hannaneh_hajishirzi.jpeg
  affiliation: University of Washington
  date: November 7, 2022
  title: "Toward Robust, Multi-Task Natural Language Processing"
  abstract: |
     Recent advances in deep learning algorithms and large-scale datasets are spurring progress in many Natural Language Processing (NLP) tasks, including question answering. Nevertheless, these models cannot scale up when task-annotated training data are scarce. This talk presents my lab's work toward building general-purpose models in NLP and how to systematically evaluate them. I present a new meta-dataset – called super-Natural Instructions –  that includes a variety of NLP tasks and their descriptions to evaluate cross-task generalization. Then, I introduce a new meta training approach that can solve more than 1600 NLP tasks only from their descriptions and a few examples. Finally, I present a series of work in robust fine-tuning methods and how to edit models with arithmetics over task vectors.

- speaker: Rui Zhang
  url: https://ryanzhumich.github.io/
  img: /assets/img/clunch/rui_zhang.jpeg
  affiliation: Penn State University
  date: October 31, 2022
  title: "Semantic Parsing in the Era of Large Language Models"
  abstract: |
     Semantic parsing is the task of translating natural language sentences into meaning representations such as SQL queries and logic forms. Traditional semantic parsing research relies on delicate data curating, heavy feature engineering, and specific model architecturing. Despite their success, these approaches are typically not generalizable across different tasks and meaning representations, limiting systematic and compatible research. In this talk, I will provide a brief overview of recent progress in unified and efficient paradigms for semantic parsing with the help of large language models (i.e., UnifiedSKG). Then, I will describe our recent work on cross-lingual semantic parsing using text-to-text language models (i.e., XSemPLR) and retrieval-augmented in-context learning (i.e., XRICL), and two new datasets to challenge the reasoning abilities of large language models on tables (i.e., MultiHiertt) and first-order logic (i.e., FOLIO). I will conclude with some future directions for semantic parsing developments.

- speaker: Xiang (Lorraine) Li
  url: https://people.cs.umass.edu/~xiangl/
  img: /assets/img/clunch/xiang_lorraine_li.jpeg
  affiliation: Allen Institute for AI (AI2) & University of Pittsburg
  date: October 24, 2022
  title: "Probabilistic Commonsense Knowledge in Language"
  abstract: |
     Commonsense knowledge is critical to achieving artificial general intelligence. This shared common background knowledge is implicit in all human communication, facilitating efficient information exchange and understanding. However, commonsense research is hampered by its immense quantity of knowledge because an explicit categorization is impossible. Furthermore, a plumber could repair a sink in a kitchen or a bathroom, indicating that common sense reveals a probable assumption rather than a definitive answer. To align with these properties of commonsense fundamentally, we want to model and evaluate such knowledge human-like using probabilistic abstractions and principles.
     This talk will introduce a probabilistic model representing commonsense knowledge using a learned latent space of geometric embeddings -- probabilistic box embeddings. Using box embeddings makes it possible to handle commonsense queries with intersections, unions, and negations in a way similar to Venn diagram reasoning. Meanwhile, existing evaluations do not reflect the probabilistic nature of commonsense knowledge. To fill in the gap, I will discuss a method of retrieving commonsense related question answer distributions from human annotators and a novel method of generative evaluation. We utilize these approaches in two new commonsense datasets (ProtoQA, Commonsense frame completion). The combination of modeling and evaluation methods based on probabilistic principles sheds light on how commonsense knowledge can be incorporated into artificial intelligence models in the future. 

- speaker: Jacob Andreas
  url: https://www.mit.edu/~jda/
  img: /assets/img/clunch/jacob_andreas.jpeg
  affiliation: MIT
  date: October 17, 2022
  title: "Toward Natural Language Supervision"
  abstract: |
     In the age of deep networks, "learning" almost invariably means "learning from examples". Image classifiers are trained with large datasets of images, machine translation systems with corpora of translated sentences, and robot policies with rollouts or demonstrations. When human learners acquire new concepts and skills, we often do so with richer supervision, especially in the form of language---we learn new concepts from exemplars accompanied by descriptions or definitions, and new skills from demonstrations accompanied by instructions. In natural language processing, recent years have seen a number of successful approaches to learning from task definitions and other forms of auxiliary language-based supervision. But these successes have been largely confined to tasks that also involve language as an input and an output---what will it take to make language-based training useful for the rest of the machine learning ecosystem? In this talk, I'll present two recent applications of natural language supervision to tasks outside the traditional domain of NLP: using language to guide visuomotor policy learning and inductive program synthesis. In these applications, natural language annotations reveal latent compositional structure in the space of programs and plans, helping models discover reusable abstractions for perception and interaction. This kind of compositional structure is present in many tasks beyond policy learning and program synthesis, and I'll conclude with a brief discussion of how these techniques might be more generally applied.

- speaker: Chenhao Tan
  url: https://chenhaot.com/
  img: /assets/img/clunch/chenhao_tan.jpeg
  affiliation: University of Chicago
  date: October 3, 2022
  title: "Towards Human-Centered Explanations of AI Predictions"
  abstract: |
     Explanations of AI predictions are considered crucial for human-AI interactions such as model debugging and model-assisted decision making, but it remains an open question what makes effective AI explanations. In this talk, I will highlight the distinction between emulation and discovery tasks, which shapes the answers to this question. In emulation tasks, humans provide groundtruth labels and the goal of AI is to emulate human intelligence. Although it is intuitive to think that humans can provide valid explanations in this case, I argue that humans may not be able to provide "good" explanations. Despite the growing efforts in building datasets of human explanations, caution is required to use such human explanations for evaluation or as supervision signals. In contrast, in discovery tasks, humans may not necessarily know the groundtruth label. While human-subject experiments are increasingly used to evaluate whether explanations improve human decisions, human+AI rarely outperforms AI alone. I will discuss the importance of identifying human strengths and AI strengths, and present our initial efforts in decision-focused summarization. I will conclude with future directions for developing effective human-centered explanations.

- speaker: William Wang (UCSB)
  url: https://sites.cs.ucsb.edu/~william/
  img: /assets/img/clunch/william_wang.png
  affiliation: UC Santa Barbara
  date: September 26, 2022
  title: "Self-Supervised Language-and-Vision Reasoning"
  abstract: |
     A key challenge for Artificial Intelligence research is to go beyond static observational data and consider more challenging settings that involve dynamic actions and incremental decision-making. In this talk, I will introduce our work on visually-grounded language reasoning via the studies of vision-and-language navigation. In particular, I will emphasize three benefits of self-supervised learning: (1) improves generalization in unseen environments; (2) creates adversarial counterfactuals to augment observational data; (3) enables transfer learning for challenging settings. I will briefly introduce other reasoning problems my groups have been working on recently.

- speaker: Michael Strube
  img: assets/img/clunch/michael_strube.jpg
  url: https://www.h-its.org/people/prof-dr-michael-strube/
  affiliation: HITS & Heidelberg University
  date: September 12, 2022
  title: Generalizability and Robustness in Coreference Resolution
  abstract: |
    In the last ten years we have seen considerable improvements in the performance of coreference resolvers, from about 60 points F1-measure to more than 80 since the CoNLL shared tasks 2011 and 2012. These improvements are mostly due to new machine learning techniques, in particular neural coreference resolvers. However, while these improvements have been reported on the CoNLL data, it is not clear whether these improvements hold on datasets in other genres, domains, and languages. In this talk I report on a series of experiments -- done by PhD. students in my research group -- testing the generalizability and robustness of coreference resolvers. Our experiments indicate that the results reported by modern machine learning based systems are not stable across genres and domains. However, the rule-based system by Lee et al. (2013), which won the CoNLL shared task 2011, is still competitive in these setups. A possible conclusion is that neural coreference resolvers should be equipped with more linguistic knowledge to make them more robust. To test the generalizability the field should not only evaluate on the CoNLL/OntoNotes data but on different domains, genres, languages and in downstream tasks.
  
