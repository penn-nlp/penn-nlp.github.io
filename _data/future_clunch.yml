# Upcoming CLunch talks can be added to this file. They will appear on the website
# in the order that they appear here, so they should be listed in
# reverse-chronological order.
#
# This should contain upcoming speakers for the current semester.
#
# Here is the template for an entry:
#
# - speaker: Name of the Speaker
#   url: http://speakers-website.com
#   affiliation: University Name
#   date: June 26, 2019
#   title: The talk of the title goes here.
#   abstract: The abstract of the talk will go here.
- speaker: Alan Ritter
  url: http://aritter.github.io
  img: assets/img/clunch/alan_ritter.JPG
  affiliation: Georgia Tech
  date: February 24th, 2023
  title: "Towards Cost Efficient Use of Pre-Trained Language Models"
  abstract: 'Large language models are leading to breakthroughs in a variety of applications, from information extraction systems that are accurate and robust, to human-like conversational assistants.  In this talk I will analyze when the benefits of training a new model outweigh the computational costs, in the context of domain adaptation.  Conventional wisdom holds that data annotation is expensive, so computational methods that leverage freely available unlabeled data can present an economical alternative when adapting to a new domain.  The talk will examine this assumption in the context of pretraining-based domain adaptation, which requires significant GPU/TPU resources for each new domain.  We frame domain adaptation as a consumer choice problem: given a fixed budget, what combination of annotation and pre-training lead to maximum utility?  In the second part of the talk, I will discuss recent work on in-context learning for anaphora resolution.  I will show that resolving anaphora in scientific protocols is a challenging task for in-context learning, then present a new method, MICE (Mixtures of In-Context Experts) and demonstrate how it can accurately resolve multiple-antecedent anaphora in paragraphs describing chemical synthesis procedures.  MICE enables accurate few-shot anaphora resolution by ensembling hundreds of prompts that are created from only a handful of training examples.  Finally, I will discuss applications of NLP on chemical synthesis protocols and show a demo of a system that can help chemists more efficiently find experimental details described in the literature.'