# Upcoming CLunch talks can be added to this file. They will appear on the website
# in the order that they appear here, so they should be listed in
# reverse-chronological order.
#
# This should contain upcoming speakers for the current semester.
#
# Here is the template for an entry:
#
# - speaker: Name of the Speaker
#   url: http://speakers-website.com
#   affiliation: University Name
#   date: June 26, 2019
#   title: The talk of the title goes here.
#   abstract: The abstract of the talk will go here.

- speaker: Sachin Kumar
  url: https://sites.google.com/view/sachinkumar
  affiliation: The Ohio State University
  date: March 17, 2025
  img: assets/img/clunch/sachin_kumar.jpg
  title: Towards Continually Adaptable Multilingual Language Models
  abstract: Subword tokenization in multilingual models presents significant challenges, particularly for non-Latin scripts and low-resource languages. Existing methods not only perform worse in these settings but also lead to higher costs in LLM APIs due to inefficiencies in token representation. Furthermore, incorporating emerging languages into pre-trained models remains difficult, with prior work often relying on heuristics like vocabulary expansion. To address these issues, I will first present MAGNET, a multilingual adaptive gradient-based tokenization algorithm that is jointly trained with a language model. Unlike standard tokenization approaches, MAGNET operates at the byte level and learns to predict segment boundaries dynamically via a tokenizer sub-module. A simple hyperparameter allows controlling over segmentation granularity, ensuring equitable treatment across languages. Its end-to-end training enables seamless adaptation of language models when fine-tuned on new languages. In the second part, I will introduce ongoing work on continually adapting multilingual language models to new languages. This work investigates how effectively models can be extended to languages with varying levels of resources and linguistic relatedness while preserving performance on previously supported languages. We study the trade-offs between adaptation efficiency, catastrophic forgetting, and cross-lingual generalization, providing insights into practical strategies for real-world multilingual NLP.

- speaker: Wei Xu
  url: https://cocoxu.github.io/
  affiliation: Georgia Institute of Technology
  date: March 24, 2025
  img: assets/img/clunch/wei_xu.jpg

- speaker: Vivek Gupta
  url: https://vgupta123.github.io/
  affiliation: Arizona State University
  date: March 31, 2025
  img: assets/img/clunch/vivek_gupta.jpg

- speaker: Leena Mathur
  url: https://l-mathur.github.io/
  affiliation: Carnegie Mellon University
  date: April 07, 2025
  img: assets/img/clunch/leena_mathur.jpg

- speaker: Yoav Artzi
  url: https://yoavartzi.com/
  affiliation: Cornell University
  date: April 14, 2025
  img: assets/img/clunch/yoav_artzi.jpg

- speaker: Daniel Khashabi
  url: https://danielkhashabi.com/
  affiliation: Johns Hopkins University
  date: April 21, 2025
  img: assets/img/clunch/daniel_khashabi.jpg

- speaker: Deen Freelon
  url: https://dfreelon.org/
  affiliation: University of Pennsylvania
  date: April 28, 2025
  img: assets/img/clunch/deen_freelon.jpg

- speaker: Jesse Thomason
  url: https://jessethomason.com/
  affiliation: University of Southern California
  date: May 05, 2025
  img: assets/img/clunch/jesse_thomason.jpg
