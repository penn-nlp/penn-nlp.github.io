# Upcoming CLunch talks can be added to this file. They will appear on the website
# in the order that they appear here, so they should be listed in
# reverse-chronological order.
#
# This should contain upcoming speakers for the current semester.
#
# Here is the template for an entry:
#
# - speaker: Name of the Speaker
#   url: http://speakers-website.com
#   affiliation: University Name
#   date: June 26, 2019
#   title: The talk of the title goes here.
#   abstract: The abstract of the talk will go here.
- speaker: Mohit Iyyer
  url: https://people.cs.umass.edu/~miyyer/
  img: assets/img/clunch/mohit_iyyer.jpeg
  affiliation: University of Massachusetts Amherst
  date: November 6
  title: Evaluating and detecting long-form LLM-generated text
  abstract: "Progress in NLP methodology over the last thirty years has been driven by benchmarks, from the Penn Treebank to GLUE. Benchmarks are useful because they provide a standard task, dataset, and means of evaluation that any researcher can use to quickly and easily demonstrate the value of their method. However, in the current age of LLMs, I argue that benchmarking is becoming increasingly obsolete. Beyond challenges such as data contamination, the dubious scientific validity of prompt engineering, and usage of closed-source APIs, each of which is critical in its own right, there exist fundamental issues with how to formulate real-world tasks into benchmarks that can rank LLMs based on the much-desired 'single score'. I highlight these issues using some of my lab's recent work on tasks such as long-form question answering, book-length summarization, and literary translation. Next, I'll pivot to a different problem that plagues not only evaluation but also society as a whole: the rapid proliferation of LLM-generated text. Detecting such text is not only important for combating malicious use cases such as academic plagiarism, but also to ensure that LLMs of the future are not just pretrained on text generated by their inferior predecessors. I outline several attacks against existing LLM-generated text detectors such as watermarking (e.g., paraphrasing, translation, cropping) and describe a retrieval-based approach that is more robust to these attacks but comes with issues of its own."